# Project Documentation for TypeScript, Crawlee, Docker, Docker Compose, and Kubernetes Deployment - UNlock

## Overview

This project is designed to perform web scraping using Crawlee, a Node.js web scraping framework, and TypeScript. The project is containerized using Docker, orchestrated with Docker Compose for local development, and deployed on Kubernetes for production environments. This document provides an in-depth explanation of each component involved, how they interact, and the steps required to build, deploy, and maintain the system.

## Key Technologies

- TypeScript: A statically typed superset of JavaScript used to write the application.
- Crawlee: A web scraping framework that simplifies the process of scraping and crawling websites.
- Docker: A platform that packages applications and their dependencies into containers, ensuring consistency across environments.
- Docker Compose: A tool for defining and running multi-container Docker applications. It allows easy orchestration of services.
- Kubernetes: An open-source system for automating the deployment, scaling, and management of containerized applications.
- TypeScript Overview: TypeScript is a programming language developed and maintained by Microsoft. It is a strict syntactical superset of JavaScript that adds optional static typing. TypeScript code is typically compiled to JavaScript for execution in environments that do not natively support TypeScript.

## Crawlee Overview

Crawlee is a powerful Node.js library that simplifies web scraping and crawling tasks. It provides high-level APIs to navigate pages, interact with dynamic content, and manage scraping tasks efficiently.

## Crawlee Features

- Request Queues: Handle and manage large numbers of URLs.
- Autoscaled Pools: Automatically scale your crawling tasks based on system resources.
- Puppeteer and Playwright Integration: Support for headless browsing to scrape dynamic content.
- Rate Limiting: Control the rate of requests to avoid being blocked by target sites.
- Project Structure
The project is organized with a clear separation between the TypeScript source code and the built JavaScript files.

```
.
├── src/                # TypeScript source files
│   ├── index.ts        # Entry point for the application
│   ├── crawler.ts      # Crawlee logic for scraping
│   └── ...             # Other source files
├── dist/               # Compiled JavaScript files (generated by the build process)
├── package.json        # NPM package configuration
├── tsconfig.json       # TypeScript configuration
└── Dockerfile          # Dockerfile to containerize the application
```

## Key TypeScript Files:

index.ts: The entry point for the application. It initializes the crawler and starts the scraping process.
crawler.ts: Contains the logic for setting up and running Crawlee, including defining routes, handling requests, and processing scraped data.

## Docker

## Docker Overview

Docker is used to containerize the application, ensuring that it runs consistently across different environments. Docker containers encapsulate the application, its dependencies, and the environment in which it runs, making it easy to develop, test, and deploy applications.

## Dockerfile Explanation

The Dockerfile is a script that defines how the Docker image for the application should be built. It uses a multi-stage build process to optimize the final image size by separating the build and runtime environments.

Dockerfile Breakdown:
```
# Specify the base Docker image. You can read more about
# the available images at https://crawlee.dev/docs/guides/docker-images
# You can also use any other image from Docker Hub.
FROM apify/actor-node:20 AS builder
Base Image: apify/actor-node:20 is used as the base image, which is optimized for Crawlee and Node.js applications.
dockerfile
Copy code
# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY package*.json ./

# Install all dependencies. Don't audit to speed up the installation.
RUN npm install --include=dev --audit=false
Dependency Installation: Only package.json and package-lock.json are copied initially to leverage Docker's layer caching, which speeds up subsequent builds if dependencies haven't changed. npm install is used to install all development dependencies, with the --audit=false flag to skip security auditing during installation for faster builds.
dockerfile
Copy code
# Next, copy the source files using the user set
# in the base image.
COPY . ./
Copy Source Code: The source files are copied into the Docker image.
dockerfile
Copy code
# Install all dependencies and build the project.
# Don't audit to speed up the installation.
RUN npm run build
Build Process: The TypeScript code is compiled to JavaScript using npm run build.
dockerfile
Copy code
# Create final image
FROM apify/actor-node:20

# Copy only built JS files from builder image
COPY --from=builder /usr/src/app/dist ./dist
Final Image: A second stage is used to create the final Docker image. This stage only includes the compiled JavaScript files (dist directory), reducing the size of the final image.
dockerfile
Copy code
# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY package*.json ./
Minimal Dependency Installation: Only production dependencies are installed to keep the image small.
dockerfile
Copy code
# Run the image.
CMD npm run start:prod --silent
```

Run Command: The application is started in production mode using npm run start:prod.
Docker Compose
Docker Compose Overview
Docker Compose is used to define and manage multi-container Docker applications. In this project, Docker Compose is configured to build and run the scraper service locally, allowing for easy orchestration and testing.

Docker Compose Configuration
The Docker Compose file (docker-compose.yml) defines the service configuration for the project.

Key Sections:
yaml
Copy code
services:
  factored-scraper:
    build:
      context: .
      dockerfile: Dockerfile
    image: factored-scraper:1.0.1
    container_name: factored-scraper-container
    platform: linux/amd64
Service Definition: The factored-scraper service is defined, specifying the build context, Dockerfile, and image name. The platform is set to linux/amd64 to ensure compatibility with the target architecture.
yaml
Copy code
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: "0.5" # Limit container to 50% of a single CPU core
          memory: "512M" # Limit container to 512MB of RAM
Resource Limits: CPU and memory limits are specified to control the resources allocated to the container. This ensures the service doesn't consume more resources than allowed, which is particularly important when running multiple instances or on shared environments.
Kubernetes Deployment
Kubernetes Overview
Kubernetes is used to manage the deployment, scaling, and operation of containerized applications in production environments. It provides mechanisms for deploying, scaling, and managing applications in a highly available and resilient manner.

Kubernetes Deployment Configuration
The Kubernetes deployment configuration (deployment.yml) defines how the scraper service is deployed and managed in a Kubernetes cluster.

Key Sections:
yaml
Copy code
apiVersion: apps/v1
kind: Deployment
metadata:
  name: factored-scraper
Deployment Definition: The deployment is named factored-scraper. This will manage the desired state of the application within the cluster.
yaml
Copy code
spec:
  replicas: 20  # Adjusted based on 10 nodes and 2 pods per node
  selector:
    matchLabels:
      app: factored-scraper
Replicas: The number of pod replicas is set to 20. This configuration is adjusted based on the assumption that the cluster has 10 nodes, and each node will run 2 pods.
yaml
Copy code
  template:
    metadata:
      labels:
        app: factored-scraper
    spec:
      containers:
        - name: factored-scraper
          image: factored-scraper:1.0.1
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: "0.5"
              memory: "512Mi"  # Adjusted based on Docker Compose
            limits:
              cpu: "1"
              memory: "1Gi"  # Adjusted based on Docker Compose
Pod Template: The pod template specifies the container that will run within each pod. The resources section is used to request and limit CPU and memory usage per container, aligning with the Docker Compose configuration.
yaml
Copy code
      # Optionally add node affinity or tolerations here if needed
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #       - matchExpressions:
      #           - key: <key>
      #             operator: In
      #             values:
      #               - <value>
Node Affinity and Tolerations: Although commented out in this configuration, node affinity and tolerations can be added to control which nodes the pods are scheduled on. This is useful for optimizing resource usage or ensuring certain workloads run on specific hardware.
Deployment Process
Build the Docker Image: Use the Dockerfile to build the container image.

bash
Copy code
docker build -t factored-scraper:1.0.1 .
Run Locally with Docker Compose: Start the service locally to ensure everything works as expected.

bash
Copy code
docker-compose up
Deploy to Kubernetes: Apply the Kubernetes deployment configuration.

bash
Copy code
kubectl apply -f deployment.yml
Monitor and Scale: Use Kubernetes tools to monitor the deployment, scale as needed, and ensure the application is running smoothly.

Conclusion
This document outlines the key components and steps required to develop, build, and deploy a web scraping service using TypeScript, Crawlee, Docker, Docker Compose, and Kubernetes. By following this guide, you can ensure that your application is developed in a scalable, maintainable, and deployable manner, suitable for both local development and production environments.
